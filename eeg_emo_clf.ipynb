{"cells":[{"cell_type":"markdown","metadata":{"id":"2fIwvzfip0dd"},"source":["# Emotion Classification"]},{"cell_type":"markdown","metadata":{"id":"2ketRUV-p0de"},"source":["### Import packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mHy8FWr0Jb01"},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import sys\n","\n","import pandas as pd\n","import numpy as np\n","\n","import pywt\n","import scipy.io as spio\n","from scipy.stats import entropy\n","from collections import Counter\n","\n","from sklearn import svm\n","from sklearn.preprocessing import normalize\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from typing import Dict, Tutple, List\n","\n","import timeit\n","import warnings\n","from sklearn.exceptions import DataConversionWarning\n","warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":90},"id":"xA-SVLx8JrV1","outputId":"7f4c850d-7761-44f4-df88-be9964df14c5"},"outputs":[{"data":{"text/plain":["'\\n   Authors: Shivam Chaudhary\\n            Centre for Brain and Cognitive Science, Indian Institute of Technology Gandhinagar \\n   In this project we will be recognising Emotion of a Human being from EEG signal.\\n   About the data set : The data set is called the seed data set.\\n   It contains data of 15 people that underwent trails 15 times each thrice.\\n\\n           Total data items =  15 (subjects) * 15 (trials each) * 3 (sessions each)\\n                            = 675 data items\\n\\n   Our project consists of 4 modules, namely : pre processing, feature extraction, feature reduction and classification,\\n   all of which are mentioned in detail in the black book.\\n\\n'"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["'''\n","   Authors: Shivam Chaudhary\n","            Centre for Brain and Cognitive Science, Indian Institute of Technology Gandhinagar\n","   In this project we will be recognising Emotion of a Human being from EEG signal.\n","   About the data set : The data set is called the seed data set.\n","   It contains data of 15 people that underwent trails 15 times each thrice.\n","\n","           Total data items =  15 (subjects) * 15 (trials each) * 3 (sessions each)\n","                            = 675 data items\n","\n","   Our project consists of 4 modules, namely : pre processing, feature extraction, feature reduction and classification,\n","   all of which are mentioned in detail in the black book.\n","\n","'''"]},{"cell_type":"markdown","metadata":{"id":"Phou7rI4p0dg"},"source":["### Define some params"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5rjOC6NHclK2"},"outputs":[],"source":["WAVELET = \"db6\"\n","MAX_LEVEL = 5"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3lnGMQvTJ9Pq"},"outputs":[],"source":["# path of .mat files\n","input_dir = \"data/eeg_raw_data/1/\"\n","# path to save our processed results including features, pc\n","output_dir = \"outputs_bp/\"\n","os.makedirs(output_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lvo7MF4OLVf_"},"outputs":[],"source":["# reading the channel order\n","channel_order = pd.read_excel(\"Channel Order.xlsx\", header=None)\n","channel_order.head()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vCjoBYD4p0dg","outputId":"fed3b3d9-bcb1-42cf-8124-dc1bf384e5d0"},"outputs":[{"data":{"text/plain":["['4_20151111.mat',\n"," '9_20151028.mat',\n"," '8_20151103.mat',\n"," '10_20151014.mat',\n"," '2_20150915.mat',\n"," '11_20150916.mat',\n"," '7_20150715.mat',\n"," '6_20150507.mat',\n"," '15_20150508.mat',\n"," '12_20150725.mat']"]},"execution_count":88,"metadata":{},"output_type":"execute_result"}],"source":["# number of input files\n","files = os.listdir(\"data/eeg_raw_data/1\")\n","files"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"f6MYlHXap0dh","outputId":"935f9635-28cb-4882-e96a-8cd5e241ed25"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>235</th>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>236</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>237</th>\n","      <td>3</td>\n","    </tr>\n","    <tr>\n","      <th>238</th>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>239</th>\n","      <td>3</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>240 rows × 1 columns</p>\n","</div>"],"text/plain":["     label\n","0        1\n","1        2\n","2        3\n","3        0\n","4        2\n","..     ...\n","235      3\n","236      0\n","237      3\n","238      0\n","239      3\n","\n","[240 rows x 1 columns]"]},"execution_count":84,"metadata":{},"output_type":"execute_result"}],"source":["# create labels dataframe\n","# it bases on len of files\n","session1_label = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3]\n","labels = session1_label * len(files)\n","labels_df = pd.DataFrame(labels)\n","labels_df.columns = ['label']\n","labels_df"]},{"cell_type":"markdown","metadata":{"id":"Ssmr7iiPdWvg"},"source":["### Feature extraction\n","\n","Now we have input data, labels, let's ready for feature extraction"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3F09QCjcp0dh"},"outputs":[],"source":["def calculate_band_power(coeff_d, band_limits):\n","    # Calculate the power spectrum of the coefficients.\n","    psd = np.abs(coeff_d)**2\n","\n","    # Calculate the band power by integrating the power spectrum within the band.\n","    band_power = np.trapz(psd, dx=(band_limits[1] - band_limits[0]))\n","\n","    return band_power"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-sLDf2Zbp0dh","outputId":"62b00a31-fd58-4d2f-8e76-6f3cba946687"},"outputs":[{"name":"stderr","output_type":"stream","text":["tqdm() Progress Bar: 100%|██████████| 10/10 [00:47<00:00,  4.74s/it]\n"]}],"source":["participant_trial = []\n","features_table = pd.DataFrame(columns=range(310))\n","\n","for file in tqdm(files, desc='tqdm() Progress Bar'):\n","    mat_file = spio.loadmat(input_dir + file)\n","    # take only signal data\n","    keys = [key for key, values in mat_file.items(\n","    ) if key != '__header__' and key != '__version__' and key != '__globals__']\n","\n","    for idx, data_file in enumerate(keys):\n","        data_df = pd.DataFrame(mat_file[data_file])\n","        channel_bps = []\n","        for channel in data_df.iterrows():\n","            dwt_bands = []\n","            data = channel[1]\n","            # mode='symmetric': The default boundary extension mode is symmetric. This means that the signal is padded with its reflection at the boundaries.\n","            for band in range(MAX_LEVEL):\n","                (data, coeff_d) = pywt.dwt(data, WAVELET)\n","                dwt_bands.append(coeff_d)\n","            band_powers = []\n","            for band in range(len(dwt_bands)):\n","                band_limits = (2**band, 2**(band + 1))\n","                band_power = calculate_band_power(dwt_bands[band], band_limits)\n","                band_powers.append(band_power)\n","            channel_bps.append(band_powers)  # 62x5\n","\n","        # Transforming 2D array into 1D vector of features\n","        unroll_bps = []\n","        for i in range(len(channel_bps)):\n","            for j in range(len(channel_bps[0])):\n","                unroll_bps.append(channel_bps[i][j])\n","\n","        participant_trial.append(unroll_bps)\n","        features_table.loc[len(features_table.index)] = unroll_bps"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BLt9Q_sYp0dh","outputId":"d7d61bef-f262-4644-cd07-2ae4e4d1aa2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 310)\n"]}],"source":["# Santity check\n","print(features_table.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L3mY47GX-tgW"},"outputs":[],"source":["# save to output dir\n","features_table.to_csv(output_dir + \"features\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"JreZ9rFodcEn"},"source":["### Principal Components Analysis"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w32UIkImbd1o"},"outputs":[],"source":["data = pd.read_csv(output_dir + \"features\" + WAVELET + \".csv\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaPLMdjfp0di"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# normalize data\n","normalised_data = pd.DataFrame(normalize(data, axis=0))\n","\n","# fit transform PCA model\n","pca_model = PCA(n_components=100)\n","components = pca_model.fit_transform(normalised_data)\n","components_df = pd.DataFrame(data=components)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0xZbenmNp0di","outputId":"934aefde-edc1-400d-e6ad-15b5d7b8efa6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0.074571</td>\n","      <td>0.262085</td>\n","      <td>0.051566</td>\n","      <td>-0.152872</td>\n","      <td>-0.062298</td>\n","      <td>0.035531</td>\n","      <td>0.034749</td>\n","      <td>-0.008298</td>\n","      <td>0.002771</td>\n","      <td>0.059400</td>\n","      <td>...</td>\n","      <td>-0.009981</td>\n","      <td>0.005770</td>\n","      <td>-0.003674</td>\n","      <td>-0.014739</td>\n","      <td>-0.001420</td>\n","      <td>-0.006554</td>\n","      <td>0.002716</td>\n","      <td>-0.003074</td>\n","      <td>-0.000340</td>\n","      <td>0.000958</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.185990</td>\n","      <td>0.044514</td>\n","      <td>0.164302</td>\n","      <td>-0.083018</td>\n","      <td>0.007252</td>\n","      <td>0.070198</td>\n","      <td>-0.009881</td>\n","      <td>-0.020256</td>\n","      <td>-0.000429</td>\n","      <td>-0.011892</td>\n","      <td>...</td>\n","      <td>0.000778</td>\n","      <td>0.010145</td>\n","      <td>-0.006817</td>\n","      <td>-0.009080</td>\n","      <td>0.002871</td>\n","      <td>-0.004952</td>\n","      <td>-0.000317</td>\n","      <td>-0.002045</td>\n","      <td>-0.003470</td>\n","      <td>0.006633</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>0.283499</td>\n","      <td>0.430817</td>\n","      <td>0.111460</td>\n","      <td>-0.185843</td>\n","      <td>-0.063564</td>\n","      <td>0.029101</td>\n","      <td>0.041894</td>\n","      <td>-0.003181</td>\n","      <td>0.018374</td>\n","      <td>0.055384</td>\n","      <td>...</td>\n","      <td>0.008095</td>\n","      <td>-0.001944</td>\n","      <td>0.001796</td>\n","      <td>0.005582</td>\n","      <td>-0.000262</td>\n","      <td>-0.008191</td>\n","      <td>-0.000938</td>\n","      <td>0.000056</td>\n","      <td>0.008669</td>\n","      <td>-0.004381</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 100 columns</p>\n","</div>"],"text/plain":["         0         1         2         3         4         5         6   \\\n","0  0.074571  0.262085  0.051566 -0.152872 -0.062298  0.035531  0.034749   \n","1 -0.185990  0.044514  0.164302 -0.083018  0.007252  0.070198 -0.009881   \n","2  0.283499  0.430817  0.111460 -0.185843 -0.063564  0.029101  0.041894   \n","\n","         7         8         9   ...        90        91        92        93  \\\n","0 -0.008298  0.002771  0.059400  ... -0.009981  0.005770 -0.003674 -0.014739   \n","1 -0.020256 -0.000429 -0.011892  ...  0.000778  0.010145 -0.006817 -0.009080   \n","2 -0.003181  0.018374  0.055384  ...  0.008095 -0.001944  0.001796  0.005582   \n","\n","         94        95        96        97        98        99  \n","0 -0.001420 -0.006554  0.002716 -0.003074 -0.000340  0.000958  \n","1  0.002871 -0.004952 -0.000317 -0.002045 -0.003470  0.006633  \n","2 -0.000262 -0.008191 -0.000938  0.000056  0.008669 -0.004381  \n","\n","[3 rows x 100 columns]"]},"execution_count":111,"metadata":{},"output_type":"execute_result"}],"source":["# Sanity check\n","components_df.head(3)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OMO99_hsi5RI"},"outputs":[],"source":["# save for reproduction\n","components_df.to_csv(output_dir + \"pc\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"ZFm_Yb9JkjUZ"},"source":["### Multiclasses classifiers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xKAodlA7p0di"},"source":["#### Data Splitting"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6CZIMZlvj4BQ"},"outputs":[],"source":["# Reading data and splitting\n","pcs = pd.read_csv(output_dir + \"pc\" + WAVELET + \".csv\")\n","# pcs = components_df\n","outputs = labels_df\n","\n","X = pcs.iloc[:, :].values\n","Y = outputs.iloc[:, :].values\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8rgOHr9Fp0di","outputId":"d2cad67c-7a8f-426e-f874-97d859ac1837"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","(240, 100) (240, 1)\n","(192, 100) (48, 100)\n","(192, 1) (48, 1)\n"]}],"source":["# Sanity check\n","print(type(X), type(Y))\n","print(X.shape, Y.shape)\n","\n","# Splitted datasets\n","print(X_train.shape, X_test.shape)\n","print(Y_train.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Vbx-SCI5p0di","outputId":"2db67c20-7b73-42eb-87ed-4c6735e68ccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 101)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>100</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 101 columns</p>\n","</div>"],"text/plain":["   100  0    1    2    3    4    5    6    7    8    ...  90   91   92   93   \\\n","0    0   60   60   60   60   60   60   60   60   60  ...   60   60   60   60   \n","1    1   60   60   60   60   60   60   60   60   60  ...   60   60   60   60   \n","2    2   60   60   60   60   60   60   60   60   60  ...   60   60   60   60   \n","3    3   60   60   60   60   60   60   60   60   60  ...   60   60   60   60   \n","\n","   94   95   96   97   98   99   \n","0   60   60   60   60   60   60  \n","1   60   60   60   60   60   60  \n","2   60   60   60   60   60   60  \n","3   60   60   60   60   60   60  \n","\n","[4 rows x 101 columns]"]},"execution_count":103,"metadata":{},"output_type":"execute_result"}],"source":["# Check data imbalanced\n","X_df = pd.DataFrame.from_records(X)\n","Y_df = pd.DataFrame.from_records(Y)\n","\n","data_merged = pd.concat([X_df, Y_df], axis=1, ignore_index=True)\n","\n","print(data_merged.shape)\n","data_merged.groupby(100).count().reset_index()\n"]},{"cell_type":"markdown","metadata":{"id":"Y9iFbXaWp0di"},"source":["#### SCV"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uJRxucIelgU1","outputId":"afc64dae-55c2-45ba-9105-6deadcd14333"},"outputs":[{"name":"stdout","output_type":"stream","text":["--- 4.970 seconds ---\n","{'C': 10000000.0, 'gamma': 1e-05}\n","Accuracy on the testing set is: 47.9%\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.88      0.64         8\n","           1       0.47      0.47      0.47        15\n","           2       0.45      0.38      0.42        13\n","           3       0.50      0.33      0.40        12\n","\n","    accuracy                           0.48        48\n","   macro avg       0.48      0.51      0.48        48\n","weighted avg       0.48      0.48      0.46        48\n","\n"]}],"source":["import warnings\n","from sklearn.exceptions import DataConversionWarning\n","warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n","\n","\n","# Create the parameter grid\n","parameters = {\n","    \"C\": (100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9),\n","    \"gamma\": (1e-08, 1e-7, 1e-6, 1e-5)\n","    }\n","\n","# Create the SVC model and grid_search object\n","svc = SVC()\n","grid_search = GridSearchCV(svc, parameters, n_jobs=-1, cv=5)\n","\n","#fit model\n","start_time = timeit.default_timer()\n","grid_search.fit(X_train, np.ravel(Y_train))\n","print(\"--- {0:.3f} seconds ---\".format(timeit.default_timer() - start_time))\n","\n","# best params\n","print(grid_search.best_params_)\n","svc_best = grid_search.best_estimator_\n","\n","accuracy = svc_best.score(X_test, Y_test)\n","print(\"Accuracy on the testing set is: {0:.1f}%\".format(accuracy*100))\n","\n","prediction = svc_best.predict(X_test)\n","\n","# confusion matrix report\n","report = classification_report(Y_test, prediction)\n","print(report)\n"]},{"cell_type":"markdown","metadata":{"id":"_KX51P0Hp0dj"},"source":["#### RandomForest"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g4OeJ6_lp0dj","outputId":"f0ade951-7c5b-4a53-8fb8-1e316e13560d"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.17      0.38      0.23         8\n","           1       0.45      0.33      0.38        15\n","           2       0.25      0.15      0.19        13\n","           3       0.27      0.25      0.26        12\n","\n","    accuracy                           0.27        48\n","   macro avg       0.29      0.28      0.27        48\n","weighted avg       0.31      0.27      0.28        48\n","\n"]}],"source":["import numpy as np\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import classification_report\n","\n","# Create the random forest classifier\n","clf_rfc = RandomForestClassifier()\n","\n","# create param grids\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'max_features': ['sqrt', 'log2'],\n","    'max_depth': [5, 10, 15],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 5]\n","}\n","\n","# create grid search object\n","grid_search = GridSearchCV(clf_rfc, param_grid, cv=5)\n","\n","# Train the classifier\n","start_time = timeit.default_timer()\n","grid_search.fit(X_train, np.ravel(Y_train))\n","print(\"--- {0:.3f} seconds ---\".format(timeit.default_timer() - start_time))\n","\n","# Make predictions on the test set\n","print(grid_search.best_params_)\n","Y_preds = grid_search.predict(X_test)\n","\n","# print classification report\n","print(classification_report(Y_test, Y_preds))"]},{"cell_type":"markdown","metadata":{"id":"VunBz2CCp0dj"},"source":["#### GradientBoosted"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xbe6w6qAp0dj","outputId":"c35b32ec-e4f7-4e26-fee1-e9f2ae479958"},"outputs":[{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.38      0.75      0.50         8\n","           1       0.70      0.47      0.56        15\n","           2       0.50      0.38      0.43        13\n","           3       0.25      0.25      0.25        12\n","\n","    accuracy                           0.44        48\n","   macro avg       0.46      0.46      0.44        48\n","weighted avg       0.48      0.44      0.44        48\n","\n"]}],"source":["import numpy as np\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","# Create the gradient boosting classifier\n","clf_gbc = GradientBoostingClassifier()\n","\n","# Create the parameter grid\n","param_grid = {\n","    'n_estimators': [100, 200, 300],\n","    'learning_rate': [0.01, 0.05, 0.1],\n","    'max_depth': [5, 10, 15],\n","    'min_samples_split': [2, 5, 10],\n","    'min_samples_leaf': [1, 2, 5]\n","}\n","\n","# Create the grid search object\n","grid_search = GridSearchCV(clf_gbc, param_grid, cv=5)\n","\n","# Train the classifier\n","start_time = timeit.default_timer()\n","grid_search.fit(X_train, np.ravel(Y_train))\n","print(\"--- {0:.3f} seconds ---\".format(timeit.default_timer() - start_time))\n","\n","# Make predictions on the test set\n","print(grid_search.best_params_)\n","Y_preds = grid_search.predict(X_test)\n","\n","# print classification report\n","print(classification_report(Y_test, Y_preds))"]},{"cell_type":"code","source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def run_experiments(df, models):\n","\n","    # Split the data into training and test sets\n","    X_train, X_test, y_train, y_test = train_test_split(df.drop('target', axis=1), df['target'], test_size=0.25)\n","\n","    # Create a grid of hyperparameters to search over\n","    param_grid = {\n","        'C': [0.1, 1, 10],\n","        'kernel': ['linear', 'rbf', 'sigmoid'],\n","    }\n","\n","    # Create a grid search object\n","    grid_search = GridSearchCV(SVC(), param_grid, cv=5)\n","\n","    # Fit the grid search object to the training set\n","    grid_search.fit(X_train, y_train)\n","\n","    # Evaluate the best model on the test set\n","    best_model = grid_search.best_estimator_\n","    score = best_model.score(X_test, y_test)\n","    print('Model: {} Score: {:.2f}'.format(best_model.__class__.__name__, score))\n","\n","    # Export the experiment results\n","    with open('results.csv', 'w') as f:\n","        writer = csv.writer(f)\n","        writer.writerow(['Model', 'Score'])\n","        for model in models:\n","            writer.writerow([model.__class__.__name__, model.score(X_test, y_test)])\n","\n","if __name__ == '__main__':\n","\n","    # Load the dataset\n","    df = pd.read_csv('data.csv')\n","\n","    # Create a list of models to test\n","    models = [SVC(), RandomForestClassifier(), GradientBoostingClassifier()]\n","\n","    # Run the experiments\n","    run_experiments(df, models)\n"],"metadata":{"id":"MitzPQtiHzaZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"r_EMM3TUp0dk"},"source":["### Findings\n","The findings from your results are that SVC has the highest f1 score of 0.6, followed by SVM with an f1 score of 0.4. Random forest and gradient boosting classifier both have an f1 score of 0.4.\n","\n","This suggests that SVC is the best performing classifier for your multiclass dataset. SVM and random forest are also performing well, but they are not as good as SVC. Gradient boosting classifier is not performing as well as the other three classifiers.\n","\n","There are a few possible reasons for these results. One possibility is that your data is non-linearly separable. SVC is a non-linear classifier, so it is better suited for this type of data. SVM and random forest are also non-linear classifiers, but they may not be as good as SVC for your particular data set. Gradient boosting classifier is a linear classifier, so it is not as good as the other three classifiers for non-linearly separable data.\n","\n","Another possibility is that your data is imbalanced. Imbalanced data means that there are more samples of one class than the other classes. SVC is not as sensitive to imbalanced data as SVM and random forest. Gradient boosting classifier is also not as sensitive to imbalanced data as SVM and random forest.\n","\n","Finally, it is also possible that the hyperparameters of the classifiers were not tuned well. Hyperparameters are the parameters that control the behavior of the classifier. They need to be tuned carefully in order to get the best performance. It is possible that the hyperparameters of the SVC, SVM, random forest, and gradient boosting classifier were not tuned well for your particular data set.\n","\n","Overall, the findings from your results suggest that SVC is the best performing classifier for your multiclass dataset. SVM and random forest are also performing well, but they are not as good as SVC. Gradient boosting classifier is not performing as well as the other three classifiers. It is possible that your data is non-linearly separable, imbalanced, or the hyperparameters of the classifiers were not tuned well."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}