{"cells":[{"cell_type":"markdown","metadata":{"id":"2fIwvzfip0dd"},"source":["# Emotion Classification"]},{"cell_type":"markdown","metadata":{"id":"2ketRUV-p0de"},"source":["### Import packages"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"mHy8FWr0Jb01"},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import sys\n","\n","import pandas as pd\n","import numpy as np\n","\n","import pywt\n","import scipy.io as spio\n","from collections import Counter\n","\n","from sklearn import svm\n","from sklearn.preprocessing import normalize\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from typing import Dict, Tuple, List\n","\n","import timeit\n","import warnings\n","from sklearn.exceptions import DataConversionWarning\n","warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"]},{"cell_type":"markdown","metadata":{"id":"Phou7rI4p0dg"},"source":["### Define some params/ args"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"3lnGMQvTJ9Pq"},"outputs":[],"source":["# path of .mat files\n","input_dir = \"data/eeg_raw_data/1/\"\n","# path to save our processed results including features, pcs\n","output_dir = \"outputs_bp/\"\n","os.makedirs(output_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lvo7MF4OLVf_"},"outputs":[{"name":"stdout","output_type":"stream","text":["(62, 1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FP1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FPZ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","0  FP1\n","1  FPZ"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# reading the channel order\n","channel_order = pd.read_excel(\"Channel Order.xlsx\", header=None)\n","print(channel_order.shape)\n","channel_order.head(2)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vCjoBYD4p0dg","outputId":"fed3b3d9-bcb1-42cf-8124-dc1bf384e5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>label_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>fear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label label_name\n","0      1        sad\n","1      2       fear"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# create labels dataframe\n","# number of input files\n","files = os.listdir(\"data/eeg_raw_data/1\")\n","# 0, 1, 2, and 3 denote the ground truth, neutral, sad, fear, and happy emotions, respectively\n","id2labels = {\n","    0: \"neutral\",\n","    1: \"sad\",\n","    2: \"fear\",\n","    3: \"happy\"\n","}\n","# create labels dataframe\n","session1_label = [1,2,3,0,2,0,0,1,0,1,2,1,1,1,2,3,2,2,3,3,0,3,0,3]\n","labels = session1_label * len(files)\n","labels_df = pd.DataFrame(labels)\n","labels_df.columns = ['label']\n","labels_df['label_name'] = labels_df['label'].map(id2labels)\n","print(labels_df.shape)\n","labels_df.head(2)"]},{"cell_type":"markdown","metadata":{"id":"Ssmr7iiPdWvg"},"source":["### Feature extraction\n","\n","Now we have input data, labels, let's ready for feature extraction. Total data points will be calculated as below:\n","- Number of participants = number of .mat files = 10\n","- Number of trials that each individual will experience = 24\n","- Number of channels = 62\n","\n","Therefore:\n","- For each participant/ a trial, eeg_signals will be collected are a matrix of (62, n), where n is the duration of the video.\n","- There will be 24 x (62, n) matrices for each participants\n","- Data will be converted into band powers of 5 sub-bands, therefore the number of features of a trial will be 62 x 5 = 310\n","- Total data points will be (10 x 24, 62 x 5) = (240, 310) "]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3F09QCjcp0dh"},"outputs":[],"source":["def calculate_band_power(coeff_d, band_limits):\n","    # Calculate the power spectrum of the coefficients.\n","    psd = np.abs(coeff_d)**2\n","\n","    # Calculate the band power by integrating the power spectrum within the band.\n","    band_power = np.trapz(psd, dx=(band_limits[1] - band_limits[0]))\n","\n","    return band_power"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-sLDf2Zbp0dh","outputId":"62b00a31-fd58-4d2f-8e76-6f3cba946687"},"outputs":[{"name":"stderr","output_type":"stream","text":["tqdm() Progress Bar:   0%|          | 0/10 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["tqdm() Progress Bar: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it]\n"]}],"source":["WAVELET = \"db6\"  # method to transform eeg signals\n","MAX_LEVEL = 5  # number of sub-bands that will be divided.\n","n_features = 310  # calculated by n_sub_bands x number of channels = 5x62\n","\n","participant_trial = []\n","features_table = pd.DataFrame(columns=range(n_features))\n","\n","for file in tqdm(files, desc='tqdm() Progress Bar'):\n","    mat_file = spio.loadmat(input_dir + file)\n","\n","    # take only signal data\n","    keys = [key for key, values in mat_file.items(\n","    ) if key != '__header__' and key != '__version__' and key != '__globals__']\n","\n","    for idx, data_file in enumerate(keys):\n","        data_df = pd.DataFrame(mat_file[data_file])\n","        channel_bps = []\n","        \n","        for channel in data_df.iterrows():\n","            dwt_bands = []\n","            data = channel[1]\n","            # mode='symmetric': The default boundary extension mode is symmetric. This means that the signal is padded with its reflection at the boundaries.\n","            for band in range(MAX_LEVEL):\n","                (data, coeff_d) = pywt.dwt(data, WAVELET)\n","                dwt_bands.append(coeff_d)\n","\n","            band_powers = []\n","            for band in range(len(dwt_bands)):\n","                band_limits = (2**band, 2**(band + 1))\n","                band_power = calculate_band_power(dwt_bands[band], band_limits)\n","                band_powers.append(band_power)\n","            channel_bps.append(band_powers)  # 62x5\n","\n","        # Transforming 2D array into 1D vector of features\n","        unroll_bps = []\n","        for i in range(len(channel_bps)):\n","            for j in range(len(channel_bps[0])):\n","                unroll_bps.append(channel_bps[i][j])\n","\n","        participant_trial.append(unroll_bps)\n","        features_table.loc[len(features_table.index)] = unroll_bps\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"BLt9Q_sYp0dh","outputId":"d7d61bef-f262-4644-cd07-2ae4e4d1aa2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 310)\n"]}],"source":["# Santity check\n","print(features_table.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"L3mY47GX-tgW"},"outputs":[],"source":["# save to output dir\n","features_table.to_csv(output_dir + \"features\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"JreZ9rFodcEn"},"source":["### Principal Components Analysis"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"w32UIkImbd1o"},"outputs":[],"source":["data = pd.read_csv(output_dir + \"features\" + WAVELET + \".csv\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"GaPLMdjfp0di"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# normalize data\n","normalised_data = pd.DataFrame(normalize(data, axis=0))\n","\n","# fit transform PCA model\n","pca_model = PCA(n_components=100)\n","components = pca_model.fit_transform(normalised_data)\n","components_df = pd.DataFrame(data=components)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0xZbenmNp0di","outputId":"934aefde-edc1-400d-e6ad-15b5d7b8efa6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.422924</td>\n","      <td>-0.140310</td>\n","      <td>0.083086</td>\n","      <td>-0.047110</td>\n","      <td>-0.003453</td>\n","      <td>0.038523</td>\n","      <td>-0.011103</td>\n","      <td>-0.019908</td>\n","      <td>0.019422</td>\n","      <td>-0.019295</td>\n","      <td>...</td>\n","      <td>0.004081</td>\n","      <td>-0.001275</td>\n","      <td>0.003786</td>\n","      <td>-0.000558</td>\n","      <td>0.003711</td>\n","      <td>-0.000079</td>\n","      <td>0.002481</td>\n","      <td>-0.002651</td>\n","      <td>0.003744</td>\n","      <td>0.004836</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.515743</td>\n","      <td>-0.257238</td>\n","      <td>0.228098</td>\n","      <td>-0.028227</td>\n","      <td>0.100964</td>\n","      <td>0.174305</td>\n","      <td>-0.390715</td>\n","      <td>2.119120</td>\n","      <td>-0.230060</td>\n","      <td>0.099261</td>\n","      <td>...</td>\n","      <td>-0.000081</td>\n","      <td>-0.000025</td>\n","      <td>-0.000011</td>\n","      <td>0.000014</td>\n","      <td>0.000021</td>\n","      <td>0.000001</td>\n","      <td>0.000022</td>\n","      <td>-0.000033</td>\n","      <td>-0.000014</td>\n","      <td>0.000016</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.330618</td>\n","      <td>-0.109024</td>\n","      <td>0.071122</td>\n","      <td>-0.011108</td>\n","      <td>-0.071077</td>\n","      <td>0.021504</td>\n","      <td>0.008361</td>\n","      <td>-0.015203</td>\n","      <td>0.000590</td>\n","      <td>-0.017432</td>\n","      <td>...</td>\n","      <td>0.006847</td>\n","      <td>-0.001291</td>\n","      <td>-0.000137</td>\n","      <td>0.006385</td>\n","      <td>-0.000426</td>\n","      <td>-0.005574</td>\n","      <td>0.000814</td>\n","      <td>-0.003628</td>\n","      <td>-0.005696</td>\n","      <td>0.000415</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 100 columns</p>\n","</div>"],"text/plain":["         0         1         2         3         4         5         6   \\\n","0 -0.422924 -0.140310  0.083086 -0.047110 -0.003453  0.038523 -0.011103   \n","1 -0.515743 -0.257238  0.228098 -0.028227  0.100964  0.174305 -0.390715   \n","2 -0.330618 -0.109024  0.071122 -0.011108 -0.071077  0.021504  0.008361   \n","\n","         7         8         9   ...        90        91        92        93  \\\n","0 -0.019908  0.019422 -0.019295  ...  0.004081 -0.001275  0.003786 -0.000558   \n","1  2.119120 -0.230060  0.099261  ... -0.000081 -0.000025 -0.000011  0.000014   \n","2 -0.015203  0.000590 -0.017432  ...  0.006847 -0.001291 -0.000137  0.006385   \n","\n","         94        95        96        97        98        99  \n","0  0.003711 -0.000079  0.002481 -0.002651  0.003744  0.004836  \n","1  0.000021  0.000001  0.000022 -0.000033 -0.000014  0.000016  \n","2 -0.000426 -0.005574  0.000814 -0.003628 -0.005696  0.000415  \n","\n","[3 rows x 100 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Sanity check\n","components_df.head(3)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OMO99_hsi5RI"},"outputs":[],"source":["# save for reproduction\n","components_df.to_csv(output_dir + \"pc\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"ZFm_Yb9JkjUZ"},"source":["### Multiclasses classifiers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xKAodlA7p0di"},"source":["#### Data Splitting"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"6CZIMZlvj4BQ"},"outputs":[],"source":["# Reading data and splitting\n","pcs = pd.read_csv(output_dir + \"pc\" + WAVELET + \".csv\")\n","# pcs = components_df\n","outputs = labels_df\n","\n","X = pcs.iloc[:, :].values\n","Y = outputs.iloc[:, :1].values\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"8rgOHr9Fp0di","outputId":"d2cad67c-7a8f-426e-f874-97d859ac1837"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","(240, 100) (240, 1)\n","(192, 100) (48, 100)\n","(192, 1) (48, 1)\n"]}],"source":["# Sanity check\n","print(type(X), type(Y))\n","print(X.shape, Y.shape)\n","\n","# Splitted datasets\n","print(X_train.shape, X_test.shape)\n","print(Y_train.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Vbx-SCI5p0di","outputId":"2db67c20-7b73-42eb-87ed-4c6735e68ccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 102)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>101</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fear</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>happy</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sad</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 102 columns</p>\n","</div>"],"text/plain":["       101  0    1    2    3    4    5    6    7    8    ...  91   92   93   \\\n","0     fear   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","1    happy   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","2  neutral   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","3      sad   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","\n","   94   95   96   97   98   99   100  \n","0   60   60   60   60   60   60   60  \n","1   60   60   60   60   60   60   60  \n","2   60   60   60   60   60   60   60  \n","3   60   60   60   60   60   60   60  \n","\n","[4 rows x 102 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Check data imbalanced\n","X_df = pd.DataFrame.from_records(X)\n","\n","data_merged = pd.concat([X_df, labels_df], axis=1, ignore_index=True)\n","# print(data_merged.head(2))\n","\n","print(data_merged.shape)\n","data_merged.groupby(101).count().reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["### Experiments"]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["def report2csv(report):\n","    report_data = []\n","    lines = report.split('\\n')\n","\n","    for line in lines[2:-5]:\n","        row_data = line.split('      ')\n","        row = {\n","            'class': row_data[1],\n","            'precision': row_data[2],\n","            'recall': row_data[3],\n","            'f1_score': row_data[4],\n","        }\n","        report_data.append(row)    \n","    report_df = pd.DataFrame.from_dict(report_data)\n","\n","    return report_df"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def run_experiments(X_train, X_test, Y_train, Y_test, param_grid, model, model_name):\n","\n","    # Create a grid search object\n","    grid_search = GridSearchCV(model, param_grid, cv=5)\n","\n","    # Fit the grid search object to the training set\n","    grid_search.fit(X_train, np.ravel(Y_train))\n","\n","    # best_params\n","    print(grid_search.best_params_)\n","    Y_pred = grid_search.predict(X_test)\n","\n","    # print classification report\n","    report = classification_report(Y_test, Y_pred)\n","    print(report)\n","    report_df = report2csv(report)\n","    report_df['model'] = model_name\n","\n","    return report_df\n"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Process model: SVC()\n","{'C': 10000000.0, 'gamma': 1e-05}\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.75      0.60         8\n","           1       0.53      0.60      0.56        15\n","           2       0.62      0.38      0.48        13\n","           3       0.55      0.50      0.52        12\n","\n","    accuracy                           0.54        48\n","   macro avg       0.55      0.56      0.54        48\n","weighted avg       0.55      0.54      0.54        48\n","\n","           0       0.50      0.75      0.60         8\n","           1       0.53      0.60      0.56        15\n","           2       0.62      0.38      0.48        13\n","           3       0.55      0.50      0.52        12\n","Process model: RandomForestClassifier()\n","{'max_depth': 12, 'n_estimators': 200}\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.75      0.55         8\n","           1       0.54      0.47      0.50        15\n","           2       0.62      0.38      0.48        13\n","           3       0.38      0.42      0.40        12\n","\n","    accuracy                           0.48        48\n","   macro avg       0.49      0.50      0.48        48\n","weighted avg       0.51      0.48      0.48        48\n","\n","           0       0.43      0.75      0.55         8\n","           1       0.54      0.47      0.50        15\n","           2       0.62      0.38      0.48        13\n","           3       0.38      0.42      0.40        12\n","Process model: GradientBoostingClassifier()\n","{'learning_rate': 0.05, 'n_estimators': 200}\n","              precision    recall  f1-score   support\n","\n","           0       0.30      0.38      0.33         8\n","           1       0.43      0.40      0.41        15\n","           2       0.60      0.46      0.52        13\n","           3       0.43      0.50      0.46        12\n","\n","    accuracy                           0.44        48\n","   macro avg       0.44      0.43      0.43        48\n","weighted avg       0.45      0.44      0.44        48\n","\n","           0       0.30      0.38      0.33         8\n","           1       0.43      0.40      0.41        15\n","           2       0.60      0.46      0.52        13\n","           3       0.43      0.50      0.46        12\n"]}],"source":["import time\n","\n","model_names = [\"SVM\", \"Random Forest\", \"Gradient Boosting\"]\n","models = [SVC(), RandomForestClassifier(), GradientBoostingClassifier()]\n","param_grids = [\n","    {\n","        \"C\": (100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9),\n","        \"gamma\": (1e-08, 1e-7, 1e-6, 1e-5)\n","    },\n","    {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [5, 10, 12],\n","    },\n","    {\n","        'n_estimators': [100, 200, 300],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","    }\n","]\n","\n","final_report_df = pd.DataFrame()\n","process_time = []\n","\n","for param_grid, model, model_name in zip(param_grids, models, model_names):\n","    start_time = time.time()\n","    print(f\"Process model: {model}\")\n","    report_df = run_experiments(X_train, X_test, Y_train, Y_test, param_grid, model, model_name)\n","    end_time = time.time()\n","    report_df['process_time'] = end_time - start_time\n","    final_report_df = pd.concat([final_report_df, report_df], axis=0, ignore_index=True)\n"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1_score</th>\n","      <th>model</th>\n","      <th>process_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.60</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.53</td>\n","      <td>0.60</td>\n","      <td>0.56</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.62</td>\n","      <td>0.38</td>\n","      <td>0.48</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.55</td>\n","      <td>0.50</td>\n","      <td>0.52</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.43</td>\n","      <td>0.75</td>\n","      <td>0.55</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0.54</td>\n","      <td>0.47</td>\n","      <td>0.50</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>0.62</td>\n","      <td>0.38</td>\n","      <td>0.48</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>0.38</td>\n","      <td>0.42</td>\n","      <td>0.40</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>0.30</td>\n","      <td>0.38</td>\n","      <td>0.33</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0.43</td>\n","      <td>0.40</td>\n","      <td>0.41</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>0.60</td>\n","      <td>0.46</td>\n","      <td>0.52</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>3</td>\n","      <td>0.43</td>\n","      <td>0.50</td>\n","      <td>0.46</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     class precision recall f1_score              model  process_time\n","0        0      0.50   0.75     0.60                SVM      0.502270\n","1        1      0.53   0.60     0.56                SVM      0.502270\n","2        2      0.62   0.38     0.48                SVM      0.502270\n","3        3      0.55   0.50     0.52                SVM      0.502270\n","4        0      0.43   0.75     0.55      Random Forest      9.089895\n","5        1      0.54   0.47     0.50      Random Forest      9.089895\n","6        2      0.62   0.38     0.48      Random Forest      9.089895\n","7        3      0.38   0.42     0.40      Random Forest      9.089895\n","8        0      0.30   0.38     0.33  Gradient Boosting     99.496126\n","9        1      0.43   0.40     0.41  Gradient Boosting     99.496126\n","10       2      0.60   0.46     0.52  Gradient Boosting     99.496126\n","11       3      0.43   0.50     0.46  Gradient Boosting     99.496126"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["final_report_df"]},{"cell_type":"code","execution_count":108,"metadata":{},"outputs":[],"source":["final_report_df.to_csv(\"outputs_bp/final_report.csv\")"]},{"cell_type":"markdown","metadata":{"id":"r_EMM3TUp0dk"},"source":["### Findings\n","The findings from your results are that SVC has the highest f1 score of 0.6, followed by SVM with an f1 score of 0.4. Random forest and gradient boosting classifier both have an f1 score of 0.4.\n","\n","This suggests that SVC is the best performing classifier for your multiclass dataset. SVM and random forest are also performing well, but they are not as good as SVC. Gradient boosting classifier is not performing as well as the other three classifiers.\n","\n","There are a few possible reasons for these results. One possibility is that your data is non-linearly separable. SVC is a non-linear classifier, so it is better suited for this type of data. SVM and random forest are also non-linear classifiers, but they may not be as good as SVC for your particular data set. Gradient boosting classifier is a linear classifier, so it is not as good as the other three classifiers for non-linearly separable data.\n","\n","Another possibility is that your data is imbalanced. Imbalanced data means that there are more samples of one class than the other classes. SVC is not as sensitive to imbalanced data as SVM and random forest. Gradient boosting classifier is also not as sensitive to imbalanced data as SVM and random forest.\n","\n","Finally, it is also possible that the hyperparameters of the classifiers were not tuned well. Hyperparameters are the parameters that control the behavior of the classifier. They need to be tuned carefully in order to get the best performance. It is possible that the hyperparameters of the SVC, SVM, random forest, and gradient boosting classifier were not tuned well for your particular data set.\n","\n","Overall, the findings from your results suggest that SVC is the best performing classifier for your multiclass dataset. SVM and random forest are also performing well, but they are not as good as SVC. Gradient boosting classifier is not performing as well as the other three classifiers. It is possible that your data is non-linearly separable, imbalanced, or the hyperparameters of the classifiers were not tuned well."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
