{"cells":[{"cell_type":"markdown","metadata":{"id":"2fIwvzfip0dd"},"source":["# Emotion Classification with Maching Learning based on EEG Signals\n","\n","### Environment\n","- Some main packages requirements:\n","\n","```\n","  - scipy==1.11.1\n","  - PyWavelets==1.4.1\n","  - scikit-learn==1.2.1\n","  - openpyxl==3.0.10\n","  - matplotlib==3.7.0\n","  - seaborn==0.12.2\n","```\n","- Create and activate new environment:\n","```\n","# Create a new environment\n","conda create --name emo python==3.9.16\n","# Activate the environement\n","conda activate emo\n","```\n","\n"]},{"cell_type":"markdown","metadata":{"id":"2ketRUV-p0de"},"source":["### Import packages"]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mHy8FWr0Jb01"},"outputs":[],"source":["from tqdm import tqdm\n","import os\n","import sys\n","\n","import pandas as pd\n","import numpy as np\n","\n","import pywt\n","import scipy.io as spio\n","from collections import Counter\n","\n","from sklearn import svm\n","from sklearn.preprocessing import normalize\n","from sklearn.model_selection import train_test_split\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.metrics import classification_report\n","from typing import Dict, Tuple, List\n","\n","import timeit\n","import warnings\n","from sklearn.exceptions import DataConversionWarning\n","warnings.filterwarnings(action='ignore', category=DataConversionWarning)\n"]},{"cell_type":"markdown","metadata":{"id":"Phou7rI4p0dg"},"source":["### Define some params/ args"]},{"cell_type":"code","execution_count":3,"metadata":{"id":"3lnGMQvTJ9Pq"},"outputs":[],"source":["# path to .mat files\n","input_dir = \"data/eeg_raw_data/1/\"\n","# path to save our processed results including features, pcs\n","output_dir = \"outputs_bp/\"\n","os.makedirs(output_dir, exist_ok=True)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"lvo7MF4OLVf_"},"outputs":[{"name":"stdout","output_type":"stream","text":["(62, 1)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>FP1</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>FPZ</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     0\n","0  FP1\n","1  FPZ"]},"execution_count":5,"metadata":{},"output_type":"execute_result"}],"source":["# reading the channel order\n","channel_order = pd.read_excel(\"Channel Order.xlsx\", header=None)\n","print(channel_order.shape)\n","channel_order.head(2)"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vCjoBYD4p0dg","outputId":"fed3b3d9-bcb1-42cf-8124-dc1bf384e5d0"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 2)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>label_name</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>1</td>\n","      <td>sad</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>2</td>\n","      <td>fear</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["   label label_name\n","0      1        sad\n","1      2       fear"]},"execution_count":6,"metadata":{},"output_type":"execute_result"}],"source":["# create labels dataframe\n","# number of input files\n","files = os.listdir(\"data/eeg_raw_data/1\")\n","# 0, 1, 2, and 3 denote the ground truth, neutral, sad, fear, and happy emotions, respectively\n","id2labels = {\n","    0: \"neutral\",\n","    1: \"sad\",\n","    2: \"fear\",\n","    3: \"happy\"\n","}\n","# create labels dataframe\n","session1_label = [1, 2, 3, 0, 2, 0, 0, 1, 0, 1,\n","                  2, 1, 1, 1, 2, 3, 2, 2, 3, 3, 0, 3, 0, 3]\n","labels = session1_label * len(files)\n","labels_df = pd.DataFrame(labels)\n","labels_df.columns = ['label']\n","labels_df['label_name'] = labels_df['label'].map(id2labels)\n","\n","# sanity check\n","print(labels_df.shape)\n","labels_df.head(2)\n"]},{"cell_type":"markdown","metadata":{"id":"Ssmr7iiPdWvg"},"source":["### Preprocessing and Feature extraction\n","\n","We have the input data and labels, and now we are ready to start preprocessing and feature extraction. The total number of data points can be calculated as follows:\n","\n","- There are 10 participants, each of which has 24 trials.\n","- Each trial has 62 channels.\n","\n","Therefore:\n","\n","- For each participant and trial, the EEG signals will be collected as a matrix of size (62, n), where `n` is the duration of the video.\n","- There will be a total of 24 x (62, n) matrices for each participant.\n","- The data will be converted into band powers of 5 sub-bands, so the number of features for each trial will be 62 x 5 = 310.\n","The total number of data points will be (10 x 24, 62 x 5) = (240, 310)."]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3F09QCjcp0dh"},"outputs":[],"source":["def calculate_band_power(coeff_d, band_limits):\n","    # Calculate the power spectrum of the coefficients.\n","    psd = np.abs(coeff_d)**2\n","\n","    # Calculate the band power by integrating the power spectrum within the band.\n","    band_power = np.trapz(psd, dx=(band_limits[1] - band_limits[0]))\n","\n","    return band_power"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"-sLDf2Zbp0dh","outputId":"62b00a31-fd58-4d2f-8e76-6f3cba946687"},"outputs":[{"name":"stderr","output_type":"stream","text":["tqdm() Progress Bar:   0%|          | 0/10 [00:00<?, ?it/s]"]},{"name":"stderr","output_type":"stream","text":["tqdm() Progress Bar: 100%|██████████| 10/10 [00:22<00:00,  2.25s/it]\n"]}],"source":["WAVELET = \"db6\"  # method to transform eeg signals\n","MAX_LEVEL = 5  # number of sub-bands that will be divided.\n","n_features = 310  # calculated by n_sub_bands x number of channels = 5x62\n","\n","participant_trial = []\n","features_table = pd.DataFrame(columns=range(n_features))\n","\n","for file in tqdm(files, desc='tqdm() Progress Bar'):\n","    mat_file = spio.loadmat(input_dir + file)\n","\n","    # take only signal data\n","    keys = [key for key, values in mat_file.items(\n","    ) if key != '__header__' and key != '__version__' and key != '__globals__']\n","\n","    for idx, data_file in enumerate(keys):\n","        data_df = pd.DataFrame(mat_file[data_file])\n","        channel_bps = []\n","\n","        for channel in data_df.iterrows():\n","            dwt_bands = []\n","            data = channel[1]\n","            # mode='symmetric': The default boundary extension mode is symmetric. This means that the signal is padded with its reflection at the boundaries.\n","            # take coeff_d of low frequency\n","            for band in range(MAX_LEVEL):\n","                (data, coeff_d) = pywt.dwt(data, WAVELET)\n","                dwt_bands.append(coeff_d)\n","\n","            band_powers = []\n","            for band in range(len(dwt_bands)):\n","                band_limits = (2**band, 2**(band + 1))\n","                band_power = calculate_band_power(dwt_bands[band], band_limits)\n","                band_powers.append(band_power)\n","            channel_bps.append(band_powers)  # 62x5\n","\n","        # Transforming 2D array into 1D vector of features\n","        unroll_bps = []\n","        for i in range(len(channel_bps)):\n","            for j in range(len(channel_bps[0])):\n","                unroll_bps.append(channel_bps[i][j])\n","\n","        participant_trial.append(unroll_bps)\n","        features_table.loc[len(features_table.index)] = unroll_bps\n"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"BLt9Q_sYp0dh","outputId":"d7d61bef-f262-4644-cd07-2ae4e4d1aa2f"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 310)\n"]}],"source":["# Santity check\n","print(features_table.shape)"]},{"cell_type":"code","execution_count":10,"metadata":{"id":"L3mY47GX-tgW"},"outputs":[],"source":["# save to output dir\n","features_table.to_csv(output_dir + \"features\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"JreZ9rFodcEn"},"source":["### Principal Components Analysis"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"w32UIkImbd1o"},"outputs":[],"source":["data = pd.read_csv(output_dir + \"features\" + WAVELET + \".csv\")"]},{"cell_type":"code","execution_count":12,"metadata":{"id":"GaPLMdjfp0di"},"outputs":[],"source":["from sklearn.decomposition import PCA\n","\n","# normalize data\n","normalised_data = pd.DataFrame(normalize(data, axis=0))\n","\n","# fit transform PCA model\n","pca_model = PCA(n_components=100)\n","components = pca_model.fit_transform(normalised_data)\n","components_df = pd.DataFrame(data=components)"]},{"cell_type":"code","execution_count":13,"metadata":{"id":"0xZbenmNp0di","outputId":"934aefde-edc1-400d-e6ad-15b5d7b8efa6"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>9</th>\n","      <th>...</th>\n","      <th>90</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>-0.422924</td>\n","      <td>-0.140310</td>\n","      <td>0.083086</td>\n","      <td>-0.047110</td>\n","      <td>-0.003453</td>\n","      <td>0.038523</td>\n","      <td>-0.011103</td>\n","      <td>-0.019908</td>\n","      <td>0.019422</td>\n","      <td>-0.019295</td>\n","      <td>...</td>\n","      <td>0.004081</td>\n","      <td>-0.001275</td>\n","      <td>0.003786</td>\n","      <td>-0.000558</td>\n","      <td>0.003711</td>\n","      <td>-0.000079</td>\n","      <td>0.002481</td>\n","      <td>-0.002651</td>\n","      <td>0.003744</td>\n","      <td>0.004836</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>-0.515743</td>\n","      <td>-0.257238</td>\n","      <td>0.228098</td>\n","      <td>-0.028227</td>\n","      <td>0.100964</td>\n","      <td>0.174305</td>\n","      <td>-0.390715</td>\n","      <td>2.119120</td>\n","      <td>-0.230060</td>\n","      <td>0.099261</td>\n","      <td>...</td>\n","      <td>-0.000081</td>\n","      <td>-0.000025</td>\n","      <td>-0.000011</td>\n","      <td>0.000014</td>\n","      <td>0.000021</td>\n","      <td>0.000001</td>\n","      <td>0.000022</td>\n","      <td>-0.000033</td>\n","      <td>-0.000014</td>\n","      <td>0.000016</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>-0.330618</td>\n","      <td>-0.109024</td>\n","      <td>0.071122</td>\n","      <td>-0.011108</td>\n","      <td>-0.071077</td>\n","      <td>0.021504</td>\n","      <td>0.008361</td>\n","      <td>-0.015203</td>\n","      <td>0.000590</td>\n","      <td>-0.017432</td>\n","      <td>...</td>\n","      <td>0.006847</td>\n","      <td>-0.001291</td>\n","      <td>-0.000137</td>\n","      <td>0.006385</td>\n","      <td>-0.000426</td>\n","      <td>-0.005574</td>\n","      <td>0.000814</td>\n","      <td>-0.003628</td>\n","      <td>-0.005696</td>\n","      <td>0.000415</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>3 rows × 100 columns</p>\n","</div>"],"text/plain":["         0         1         2         3         4         5         6   \\\n","0 -0.422924 -0.140310  0.083086 -0.047110 -0.003453  0.038523 -0.011103   \n","1 -0.515743 -0.257238  0.228098 -0.028227  0.100964  0.174305 -0.390715   \n","2 -0.330618 -0.109024  0.071122 -0.011108 -0.071077  0.021504  0.008361   \n","\n","         7         8         9   ...        90        91        92        93  \\\n","0 -0.019908  0.019422 -0.019295  ...  0.004081 -0.001275  0.003786 -0.000558   \n","1  2.119120 -0.230060  0.099261  ... -0.000081 -0.000025 -0.000011  0.000014   \n","2 -0.015203  0.000590 -0.017432  ...  0.006847 -0.001291 -0.000137  0.006385   \n","\n","         94        95        96        97        98        99  \n","0  0.003711 -0.000079  0.002481 -0.002651  0.003744  0.004836  \n","1  0.000021  0.000001  0.000022 -0.000033 -0.000014  0.000016  \n","2 -0.000426 -0.005574  0.000814 -0.003628 -0.005696  0.000415  \n","\n","[3 rows x 100 columns]"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["# Sanity check\n","components_df.head(3)"]},{"cell_type":"code","execution_count":14,"metadata":{"id":"OMO99_hsi5RI"},"outputs":[],"source":["# save for reproduction\n","components_df.to_csv(output_dir + \"pc\" + WAVELET + \".csv\", index=False)"]},{"cell_type":"markdown","metadata":{"id":"ZFm_Yb9JkjUZ"},"source":["### Multiclasses classifiers\n","\n"]},{"cell_type":"markdown","metadata":{"id":"xKAodlA7p0di"},"source":["#### Data Splitting"]},{"cell_type":"code","execution_count":15,"metadata":{"id":"6CZIMZlvj4BQ"},"outputs":[],"source":["# Reading data and splitting\n","pcs = pd.read_csv(output_dir + \"pc\" + WAVELET + \".csv\")\n","# pcs = components_df\n","outputs = labels_df\n","\n","X = pcs.iloc[:, :].values\n","Y = outputs.iloc[:, :1].values\n","\n","X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.2, random_state=42)"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"8rgOHr9Fp0di","outputId":"d2cad67c-7a8f-426e-f874-97d859ac1837"},"outputs":[{"name":"stdout","output_type":"stream","text":["<class 'numpy.ndarray'> <class 'numpy.ndarray'>\n","(240, 100) (240, 1)\n","(192, 100) (48, 100)\n","(192, 1) (48, 1)\n"]}],"source":["# Sanity check\n","print(type(X), type(Y))\n","print(X.shape, Y.shape)\n","\n","# Splitted datasets\n","print(X_train.shape, X_test.shape)\n","print(Y_train.shape, Y_test.shape)"]},{"cell_type":"code","execution_count":17,"metadata":{"id":"Vbx-SCI5p0di","outputId":"2db67c20-7b73-42eb-87ed-4c6735e68ccf"},"outputs":[{"name":"stdout","output_type":"stream","text":["(240, 102)\n"]},{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>101</th>\n","      <th>0</th>\n","      <th>1</th>\n","      <th>2</th>\n","      <th>3</th>\n","      <th>4</th>\n","      <th>5</th>\n","      <th>6</th>\n","      <th>7</th>\n","      <th>8</th>\n","      <th>...</th>\n","      <th>91</th>\n","      <th>92</th>\n","      <th>93</th>\n","      <th>94</th>\n","      <th>95</th>\n","      <th>96</th>\n","      <th>97</th>\n","      <th>98</th>\n","      <th>99</th>\n","      <th>100</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>fear</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>happy</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>neutral</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>sad</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>...</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","      <td>60</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>4 rows × 102 columns</p>\n","</div>"],"text/plain":["       101  0    1    2    3    4    5    6    7    8    ...  91   92   93   \\\n","0     fear   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","1    happy   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","2  neutral   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","3      sad   60   60   60   60   60   60   60   60   60  ...   60   60   60   \n","\n","   94   95   96   97   98   99   100  \n","0   60   60   60   60   60   60   60  \n","1   60   60   60   60   60   60   60  \n","2   60   60   60   60   60   60   60  \n","3   60   60   60   60   60   60   60  \n","\n","[4 rows x 102 columns]"]},"execution_count":17,"metadata":{},"output_type":"execute_result"}],"source":["# Check data imbalanced\n","X_df = pd.DataFrame.from_records(X)\n","\n","data_merged = pd.concat([X_df, labels_df], axis=1, ignore_index=True)\n","# print(data_merged.head(2))\n","\n","print(data_merged.shape)\n","data_merged.groupby(101).count().reset_index()"]},{"cell_type":"markdown","metadata":{},"source":["#### Experiments\n","\n","Data is ready, let's start with machine learning methods as we have tabular data at hands. There are 3 models that will be experiments which is SVM, RandomForestClassifier and GradientBoostedClassifier."]},{"cell_type":"code","execution_count":63,"metadata":{},"outputs":[],"source":["def report2csv(report):\n","    \"\"\"Convert classification report to dataframe\n","    \"\"\"\n","    report_data = []\n","    # split into lines\n","    lines = report.split('\\n')\n","    # take report per each class\n","    for line in lines[2:-5]:\n","        row_data = line.split('      ')\n","        row = {\n","            'class': row_data[1],\n","            'precision': row_data[2],\n","            'recall': row_data[3],\n","            'f1_score': row_data[4],\n","        }\n","        report_data.append(row)\n","    report_df = pd.DataFrame.from_dict(report_data)\n","\n","    return report_df\n"]},{"cell_type":"code","execution_count":90,"metadata":{},"outputs":[],"source":["import pandas as pd\n","from sklearn.model_selection import train_test_split, GridSearchCV\n","from sklearn.svm import SVC\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.ensemble import GradientBoostingClassifier\n","\n","def run_experiments(X_train, X_test, Y_train, Y_test, param_grid, model, model_name):\n","\n","    # Create a grid search object\n","    grid_search = GridSearchCV(model, param_grid, cv=5)\n","\n","    # Fit the grid search object to the training set\n","    grid_search.fit(X_train, np.ravel(Y_train))\n","\n","    # Best_params\n","    print(grid_search.best_params_)\n","    Y_pred = grid_search.predict(X_test)\n","\n","    # Print classification report\n","    report = classification_report(Y_test, Y_pred)\n","    print(report)\n","    report_df = report2csv(report)\n","    report_df['model'] = model_name\n","\n","    return report_df\n"]},{"cell_type":"code","execution_count":106,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Process model: SVC()\n","{'C': 10000000.0, 'gamma': 1e-05}\n","              precision    recall  f1-score   support\n","\n","           0       0.50      0.75      0.60         8\n","           1       0.53      0.60      0.56        15\n","           2       0.62      0.38      0.48        13\n","           3       0.55      0.50      0.52        12\n","\n","    accuracy                           0.54        48\n","   macro avg       0.55      0.56      0.54        48\n","weighted avg       0.55      0.54      0.54        48\n","\n","           0       0.50      0.75      0.60         8\n","           1       0.53      0.60      0.56        15\n","           2       0.62      0.38      0.48        13\n","           3       0.55      0.50      0.52        12\n","Process model: RandomForestClassifier()\n","{'max_depth': 12, 'n_estimators': 200}\n","              precision    recall  f1-score   support\n","\n","           0       0.43      0.75      0.55         8\n","           1       0.54      0.47      0.50        15\n","           2       0.62      0.38      0.48        13\n","           3       0.38      0.42      0.40        12\n","\n","    accuracy                           0.48        48\n","   macro avg       0.49      0.50      0.48        48\n","weighted avg       0.51      0.48      0.48        48\n","\n","           0       0.43      0.75      0.55         8\n","           1       0.54      0.47      0.50        15\n","           2       0.62      0.38      0.48        13\n","           3       0.38      0.42      0.40        12\n","Process model: GradientBoostingClassifier()\n","{'learning_rate': 0.05, 'n_estimators': 200}\n","              precision    recall  f1-score   support\n","\n","           0       0.30      0.38      0.33         8\n","           1       0.43      0.40      0.41        15\n","           2       0.60      0.46      0.52        13\n","           3       0.43      0.50      0.46        12\n","\n","    accuracy                           0.44        48\n","   macro avg       0.44      0.43      0.43        48\n","weighted avg       0.45      0.44      0.44        48\n","\n","           0       0.30      0.38      0.33         8\n","           1       0.43      0.40      0.41        15\n","           2       0.60      0.46      0.52        13\n","           3       0.43      0.50      0.46        12\n"]}],"source":["import time\n","\n","model_names = [\"SVM\", \"Random Forest\", \"Gradient Boosting\"]\n","models = [SVC(), RandomForestClassifier(), GradientBoostingClassifier()]\n","param_grids = [\n","    {\n","        \"C\": (100, 1e3, 1e4, 1e5, 1e6, 1e7, 1e8, 1e9),\n","        \"gamma\": (1e-08, 1e-7, 1e-6, 1e-5)\n","    },\n","    {\n","        'n_estimators': [100, 200, 300],\n","        'max_depth': [5, 10, 12],\n","    },\n","    {\n","        'n_estimators': [100, 200, 300],\n","        'learning_rate': [0.01, 0.05, 0.1],\n","    }\n","]\n","\n","final_report_df = pd.DataFrame()\n","process_time = []\n","\n","for param_grid, model, model_name in zip(param_grids, models, model_names):\n","    start_time = time.time()\n","    print(f\"Process model: {model}\")\n","    report_df = run_experiments(\n","        X_train, X_test, Y_train, Y_test, param_grid, model, model_name)\n","    end_time = time.time()\n","    report_df['process_time'] = end_time - start_time\n","    final_report_df = pd.concat(\n","        [final_report_df, report_df], axis=0, ignore_index=True)\n"]},{"cell_type":"code","execution_count":107,"metadata":{},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>class</th>\n","      <th>precision</th>\n","      <th>recall</th>\n","      <th>f1_score</th>\n","      <th>model</th>\n","      <th>process_time</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>0</td>\n","      <td>0.50</td>\n","      <td>0.75</td>\n","      <td>0.60</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>1</td>\n","      <td>0.53</td>\n","      <td>0.60</td>\n","      <td>0.56</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>2</td>\n","      <td>0.62</td>\n","      <td>0.38</td>\n","      <td>0.48</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>3</td>\n","      <td>0.55</td>\n","      <td>0.50</td>\n","      <td>0.52</td>\n","      <td>SVM</td>\n","      <td>0.502270</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>0</td>\n","      <td>0.43</td>\n","      <td>0.75</td>\n","      <td>0.55</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>5</th>\n","      <td>1</td>\n","      <td>0.54</td>\n","      <td>0.47</td>\n","      <td>0.50</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>6</th>\n","      <td>2</td>\n","      <td>0.62</td>\n","      <td>0.38</td>\n","      <td>0.48</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>3</td>\n","      <td>0.38</td>\n","      <td>0.42</td>\n","      <td>0.40</td>\n","      <td>Random Forest</td>\n","      <td>9.089895</td>\n","    </tr>\n","    <tr>\n","      <th>8</th>\n","      <td>0</td>\n","      <td>0.30</td>\n","      <td>0.38</td>\n","      <td>0.33</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>9</th>\n","      <td>1</td>\n","      <td>0.43</td>\n","      <td>0.40</td>\n","      <td>0.41</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2</td>\n","      <td>0.60</td>\n","      <td>0.46</td>\n","      <td>0.52</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","    <tr>\n","      <th>11</th>\n","      <td>3</td>\n","      <td>0.43</td>\n","      <td>0.50</td>\n","      <td>0.46</td>\n","      <td>Gradient Boosting</td>\n","      <td>99.496126</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["     class precision recall f1_score              model  process_time\n","0        0      0.50   0.75     0.60                SVM      0.502270\n","1        1      0.53   0.60     0.56                SVM      0.502270\n","2        2      0.62   0.38     0.48                SVM      0.502270\n","3        3      0.55   0.50     0.52                SVM      0.502270\n","4        0      0.43   0.75     0.55      Random Forest      9.089895\n","5        1      0.54   0.47     0.50      Random Forest      9.089895\n","6        2      0.62   0.38     0.48      Random Forest      9.089895\n","7        3      0.38   0.42     0.40      Random Forest      9.089895\n","8        0      0.30   0.38     0.33  Gradient Boosting     99.496126\n","9        1      0.43   0.40     0.41  Gradient Boosting     99.496126\n","10       2      0.60   0.46     0.52  Gradient Boosting     99.496126\n","11       3      0.43   0.50     0.46  Gradient Boosting     99.496126"]},"execution_count":107,"metadata":{},"output_type":"execute_result"}],"source":["final_report_df"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["final_report_df.to_csv(\"outputs_bp/final_report.csv\")"]},{"cell_type":"markdown","metadata":{"id":"r_EMM3TUp0dk"},"source":["### Findings\n","#### Experiment Results\n","\n","- The SVM model has the highest F1 score for class 0 (0.60), followed by the Random Forest model (0.55) and the Gradient Boosting model (0.52).\n","- The SVM model has the highest precision for class 2 (0.62), followed by the Random Forest model (0.60) and the Gradient Boosting model (0.48).\n","- The SVM model has the highest recall for class 1 (0.75), followed by the Random Forest model (0.60) and the Gradient Boosting model (0.47).\n","- The Gradient Boosting model has the slowest training time, followed by the SVM model and the Random Forest model.\n","\n","Overall, the SVM model seems to be the best performing model, followed by the Random Forest model and the Gradient Boosting model. Dataset is balanced so models are not biased towards any particular class. However, the F1 score is not really high. \n","\n","#### Improvement Points\n","\n","Based on the experimental results, here are some points that could help improve the model:\n","\n","- Even after reducing the number of features to 100 principal components (PCs), the number of features is still high for machine learning models, as the dataset is not large. Collecting more data would help improve this.\n","- Only band power is currently calculated to convert into features to feed into machine learning models. We can improve the model by finding more relevant features, such as band power.\n","- I have already tried using grid search to find the best parameters for machine learning models, but I did not have enough time to experiment with many parameters. Fine-tuning these hyperparameters would help improve the model.\n","- Finally, trying some basic deep learning models would also be helpful."]}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.16"}},"nbformat":4,"nbformat_minor":0}
